{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjRYaO1LrglTgTcYlCa5yj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/8131maggie/kaggle/blob/master/titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkiuK23QeLkT",
        "colab_type": "text"
      },
      "source": [
        "# 下準備\n",
        "keggleのタイタニックcompetition/data のページ(\n",
        "https://www.kaggle.com/c/titanic/data) から   \n",
        "・test.csv   \n",
        "・train.csv   \n",
        "上記2ファイルをダウンロードし、Google-colab上にアップロードする(ローカルからドラッグアンドドロップでOK)\n",
        "\n",
        "後は下のコードを実行すればkaggleにsubmitできる   \n",
        "・submission.csv   \n",
        "というファイルが作成されるのでkaggleに提出する\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEtgPNPgaQ5I",
        "colab_type": "code",
        "outputId": "d5f22988-04f8-4501-ebe3-2f0bb3a1af2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Use LGBM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "test_passenger_ids = test_df.pop('PassengerId')\n",
        "train_df.drop(['PassengerId'], axis=1, inplace=True)\n",
        "\n",
        "df_list = [train_df, test_df]\n",
        "\n",
        "for df in df_list:    \n",
        "    \n",
        "    # Transform 'Sex'\n",
        "    df.loc[df['Sex'] == 'female','Sex'] = 0\n",
        "    df.loc[df['Sex'] == 'male','Sex'] = 1\n",
        "    df['Sex'] = df['Sex'].astype('int8')\n",
        "\n",
        "    df.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
        "\n",
        "y = train_df.pop('Survived')\n",
        "\n",
        "# Take a hold out set randomly\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an LGBM dataset for training\n",
        "categorical_features = ['Age', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Fare']\n",
        "train_data = lgb.Dataset(data=X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=False)\n",
        "\n",
        "# Create an LGBM dataset from the test\n",
        "test_data = lgb.Dataset(data=X_test, label=y_test, categorical_feature=categorical_features, free_raw_data=False)\n",
        "\n",
        "# Finally, create a dataset for the FULL training data to give us maximum amount of data to train on after \n",
        "# performance has been calibrate\n",
        "final_train_set = lgb.Dataset(data=train_df, label=y, \n",
        "                               categorical_feature=categorical_features, free_raw_data=False)\n",
        "\n",
        "lgb_params = {\n",
        "    'boosting': 'dart',          # dart (drop out trees) often performs better\n",
        "    'application': 'binary',     # Binary classification\n",
        "    'learning_rate': 0.05,       # Learning rate, controls size of a gradient descent step\n",
        "    'min_data_in_leaf': 20,      # Data set is quite small so reduce this a bit\n",
        "    'feature_fraction': 0.7,     # Proportion of features in each boost, controls overfitting\n",
        "    'num_leaves': 41,            # Controls size of tree since LGBM uses leaf wise splits\n",
        "    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric\n",
        "    'drop_rate': 0.2,            # Dropout probability\n",
        "              }\n",
        "\n",
        "\n",
        "evaluation_results = {}\n",
        "clf = lgb.train(train_set=train_data,\n",
        "                 params=lgb_params,\n",
        "                 valid_sets=[train_data, test_data], \n",
        "                 valid_names=['Train', 'Test'],\n",
        "                 evals_result=evaluation_results,\n",
        "                 num_boost_round=500,\n",
        "                 early_stopping_rounds=100,\n",
        "                 verbose_eval=50\n",
        "                )\n",
        "\n",
        "optimum_boost_rounds = clf.best_iteration\n",
        "\n",
        "preds = np.round(clf.predict(X_test))\n",
        "print('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\n",
        "print('Precision score = \\t {}'.format(precision_score(y_test, preds)))\n",
        "print('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\n",
        "print('F1 score =      \\t {}'.format(f1_score(y_test, preds)))\n",
        "\n",
        "clf_final = lgb.train(train_set=final_train_set,\n",
        "                      params=lgb_params,\n",
        "                      num_boost_round=500,                    \n",
        "                      )\n",
        "\n",
        "y_pred = np.round(clf_final.predict(test_df)).astype(int)\n",
        "\n",
        "output_df = pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': y_pred})\n",
        "output_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
            "  warnings.warn('Early stopping is not available in dart mode')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[50]\tTrain's binary_logloss: 0.502753\tTest's binary_logloss: 0.508898\n",
            "[100]\tTrain's binary_logloss: 0.478109\tTest's binary_logloss: 0.489976\n",
            "[150]\tTrain's binary_logloss: 0.459353\tTest's binary_logloss: 0.474709\n",
            "[200]\tTrain's binary_logloss: 0.441243\tTest's binary_logloss: 0.465009\n",
            "[250]\tTrain's binary_logloss: 0.443344\tTest's binary_logloss: 0.470668\n",
            "[300]\tTrain's binary_logloss: 0.42155\tTest's binary_logloss: 0.463775\n",
            "[350]\tTrain's binary_logloss: 0.410914\tTest's binary_logloss: 0.464199\n",
            "[400]\tTrain's binary_logloss: 0.403528\tTest's binary_logloss: 0.462805\n",
            "[450]\tTrain's binary_logloss: 0.39807\tTest's binary_logloss: 0.464639\n",
            "[500]\tTrain's binary_logloss: 0.388235\tTest's binary_logloss: 0.469024\n",
            "Accuracy score = \t 0.8156424581005587\n",
            "Precision score = \t 0.8253968253968254\n",
            "Recall score =   \t 0.7027027027027027\n",
            "F1 score =      \t 0.7591240875912408\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}